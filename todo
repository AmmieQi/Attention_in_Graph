tune labmda in w_a w_b sum
masked batch norm

dd dropout

dd 5-10 layer relu
